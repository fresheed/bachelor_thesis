\subsection{Наивный байесовский классификатор}

В основе данного метода лежит формула Байеса:

$P(Y_i|X)=\dfrac{P(X|Y)P(Y)}{P(X)}$

Здесь $P(X|Y_i)$ - вероятность найти элемент $X$ в классе $Y_i$, $P(Y_i)$ - вероятность найти какой-то элемент класса $Y_i$ среди всего множества элементов, $P(X)$ - вероятность найти элемент $X$ среди всего множества элементов, $P(Y_i|X)$ - вероятность того, что данный элемент $X$ принадлежит классу $Y_i$.

Задача классификации в этом случае сводится к выбору класса $Y_{opt}$, который максимизирует вероятность принадлежности к этому классу: $Y_{opt}=argmax_{Y_i} P(Y_i|X)=argmax_{Y_i} \dfrac{P(X|Y)P(Y)}{P(X)}$.

Так как $P(X)$ является для данного элемента константой, в расчётах её можно не учитывать. $P(Y_i)$ можно рассчитать как отношение числа элементов класса $Y_i$ к общему числу элементов выборки. 

Для расчёта $P(X|Y_i)$ необходимо представить $X$ как набор признаков: $P(X|Y_i)=P(c_1, c_2 .. c_m | Y_i)$, где набор $c_j$ - множество всех признаков объекта. В общем случае эта вероятность рассчитывается с учётом зависимостей между признаками: $P(X|Y_i)=P(c_1|Y_i)*P(c_2|c_1,Y_i)*..*P(c_m|c_1,..c_{m-1},Y_i)$. Восстановление зависимостей между признаками по обучающей выборке - достаточно трудная задача.

Метод наивной байесовской классификации предполагает, что признаки являются независимыми: $P(c_2|c_1,Y_i)=P(c_2|Y_i)$ и т.д. Тогда для расчёта $P(X|Y_i)$ необходимо рассчитать $m$ вероятностей того, что $j$ признак примет значение $c_j$ при условии принадлежности элемента к классу $Y_i$. На практике это предположение справедливо далеко не всегда, однако эксперименты показали, что это упрощение позволяет эффективно решать многие задачи.

В решаемой задаче $c_j$ можно рассматривать как непрерывные величины. В общем случае $P(c_j|Y_i)$ получаются в результате выполнения процедуры оценивания плотности. Один из способов рассчитать $P(c_j|Y_i)$ - предположить, что значения признаков распределены по Гауссу и использовать формулу плотности нормального распределения\cite{sklearn_gnb}:

$P(c_j|Y_i)=\dfrac{1}{\sqrt{2\pi\sigma_i^2}}*exp(-\dfrac{(c_j-\mu_i)^2}{2\sigma_i^2})$.

В процессе обучения параметры распределения $\mu_i$ и $\sigma_i^2$ рассчитываются по значениям $c_j$ из обучающей выборки. 

% http://bazhenov.me/blog/2012/06/11/naive-bayes.html
% http://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes
% http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf - пока что не использовано