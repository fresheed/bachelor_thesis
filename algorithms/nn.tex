
\subsection{Нейронные сети}

% Нейронная сеть представляют собой множество связанных между собой элементарных вычислителей - нейронов, . Их можно 

Аппарат искусственных нейронных сетей позволяет представить решение некоторой задачи как совокупность связей между элементарными вычислителями - нейронами. Эта идея напоминает суть организации мозга - знания в нём также представлены синаптическими связями между нейронами. 

Отдельный нейрон представляет собой единицу обработки информации в сети. Он выполняет взвешенное суммирование входных сигналов, и полученная величина обрабатывается некоторой функцией активации (см. рис. \ref{fig:neuron})\cite{haykin_nn}: 

$y_k=\varphi(\sum\limits_{j=1}^m w_{kj}*x_j)$

\pastepic{Структура нейрона}{design/neuron.png}{neuron}


Если в качестве функции активации взять, например, функцию Хевисайда 
\[
  \varphi(u)=\begin{cases}
               1, u>=0\\
               0, u<0 \\
              \end{cases}
\], то при правильной настройке весов нейрон может моделировать такие логические функции, как AND, OR, NOT (рис. \ref{fig:neuron_logic}\cite{neuron_logic}; $T$ обозначает величину порога активации):

\pastepicthree{Моделирование логических функций AND, OR, NOT с помощью нейрона}{design/neuron_and.png}{design/neuron_or.png}{design/neuron_not.png}{neuron_logic}

Однако функция сложения по модулю 2 нейрон промоделировать уже не может. Это связано с тем, что нейрон фактически проводит в N-мерном пространстве гиперплоскость, разделяющую объекты двух классов. Для функции XOR такое разделение невозможно. Тем не менее, эта задача может быть решена композицией нейронов (рис. \ref{fig:neuron_xor}). Это свойство - повышение вычислительной мощности с усложнением структуры сети - позволяет создавать сети, моделирующие достаточно сложные системы. 

\pastepic{Решение задачи XOR нейронной сетью}{design/neuron_xor.png}{neuron_xor}


Для задач классификации наиболее распространённой является архитектура многослойного персептрона\cite{haykin_nn}:

\begin{itemize}
\item нейроны имеют нелинейную дифференцируемую функцию активации
\item в сети есть один или более скрытых слоёв (не являющихся входом или выходом сети)
\item сеть является полносвязной - каждый нейрон слоя связан со всеми нейронами следующего слоя
\end{itemize}

Для решения задачи мультиклассового распознавания выходной слой сети реализуют с использованием функции softmax: $\varphi_i(u)=\dfrac{exp(u_i)}{\sum\limits_{j=1}^N exp(u_j)}$, где $N$ - количество нейронов выходного слоя, равное числу классов. Таким образом, сумма значений, получаемых с выхода сети, равна единице, и совокупность этих значений можно рассматривать как вероятности принадлежности входного вектора тому или иному классу.

Наиболее простым алгоритмом обучения (подбора значений весов) нейронной сети является алгоритм обратного распространения ошибки\cite{haykin_nn}. На практике применяются его модификации, повышающие скорость схождения к оптимальным значениям.

