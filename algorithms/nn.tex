
\subsection{Нейронные сети}

% Нейронная сеть представляют собой множество связанных между собой элементарных вычислителей - нейронов, . Их можно 

Аппарат искусственных нейронных сетей позволяет представить решение некоторой задачи как совокупность связей между элементарными вычислителями - нейронами. Эта идея напоминает суть организации мозга - знания в нём также представлены синаптическими связями между нейронами. 

Отдельный нейрон представляет собой единицу обработки информации в сети. Он выполняет взвешенное суммирование входных сигналов, и полученная величина обрабатывается некоторой функцией активации (см. рис. \ref{fig:neuron}): 

$y_k=\varphi(\sum_{j=1}^m w_{kj}*x_j)$

\pastepic{Структура нейрона}{design/neuron.png}{neuron}

\missing{ссылка на хайкина}

Если в качестве функции активации взять, например, функцию Хевисайда 
\[
  \varphi(u)=\begin{cases}
               1, u>=0\\
               0, u<0 \\
              \end{cases}
\], то при правильной настройке весов нейрон может моделировать такие логические функции, как AND, OR, NOT (рис. \ref{fig:neuron_logic}):

%https://ru.wikipedia.org/wiki/%D0%98%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD
\pastepicthree{Моделирование логических функций AND, OR, NOT с помощью нейрона}{design/neuron_and.png}{design/neuron_or.png}{design/neuron_not.png}{neuron_logic}

\missing{ссылка на википедию}

\missing{поправить вёрстку пикчи}

Однако функция сложения по модулю 2 нейрон промоделировать уже не может. Это связано с тем, что нейрон фактически проводит в N-мерном пространстве гиперплоскость, разделяющую объекты двух классов. Для функции XOR такое разделение невозможно. Тем не менее, эта задача может быть решена композицией нейронов (рис. \ref{fig:neuron_xor}). Это свойство - повышение вычислительной мощности с усложнением структуры сети - позволяет создавать сети, моделирующие достаточно сложные системы. 

\pastepic{Решение задачи XOR нейронной сетью}{design/neuron_xor.png}{neuron_xor}


Для задач классификации наиболее распространённой является архитектура многослойного персептрона\missing{ссылка на хайкина}:

\begin{itemize}
\item нейроны имеют нелинейную дифференцируемую функцию активации
\item в сети есть один или более скрытых слоёв (не являющихся входом или выходом сети)
\item сеть является полносвязной - каждый нейрон слоя связан со всеми нейронами следующего слоя
\end{itemize}

Для решения задачи мультиклассового распознавания выходной слой сети реализуют с использованием функции softmax: $\varphi_i(u)=\dfrac{exp(u_i)}{\sum_{j=1}^N exp(u_j)}$, где $N$ - количество нейронов выходного слоя, равное числу классов. Таким образом, сумма значений, получаемых с выхода сети, равна единице, и совокупность этих значений можно рассматривать как вероятности принадлежности входного вектора тому или иному классу.

Наиболее простым алгоритмом обучения (подбора значений весов) нейронной сети является алгоритм обратного распространения ошибки.

\missing{нужно ли приводить алгоритм?}

