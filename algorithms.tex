
\chapter{Обзор алгоритмов решения задачи}

В наиболее абстрактном виде задача описывается следующим образом. 

Временные ряды представлены конечными последовательностями значений: $S=s_1,s_2...s_n$, где $s_i$ - вектор значений $\left\{x_i,y_i,z_i\right\}$. Дан конечный набор классов $C=c_1,c_2...c_m$. Известно, что каждый ряд принадлежит одному и только одному классу. 

Для некоторого множества $L$ рядов известна принадлежность ряда к классу - такое множество назовём \defn{обучающей выборкой}. Также имеется множество $T$ рядов, принадлежность которых к классам неизвестна - это \defn{тестовая выборка}.

Необходимо по данной обучающей выборке классифицировать элементы тестовой выборки с максимальной точностью.

\missing{confmat ?}. 

\missing{другие метрики}


\section{Алгоритмы, основанные на метриках расстояния}

Существует класс алгоритмов, основанных на вычислении расстояния (\defn{метрики}) между классифицируемыми объектами. 

\subsection{Алгоритм динамической трансформации временной шкалы (DTW)}

Основная идея алгоритма DTW - минимизация эффекта сдвигов и искажений сигналов во времени путём трансформации одного временного ряда в другой. Это позволяет находить сигналы схожих форм с разными фазами\cite{dtw_review}. 

Пусть даны два временных ряда $X=x_1,x_2...x_N$ и $Y=y_1,y_2...y_M$. Элементы рядов $x_i,y_j$ принадлежат одному пространству \missing{уточнить} $\Phi$, для которого введена метрика $d: \Phi \times \Phi \to R$. 

Алгоритм DTW заключается в следующем:

\begin{enumerate}
\item Строится матрица $D$, состоящая из значений метрик для всех возможных пар элементов рядов: $d_{ij}=|| x_i-y_j ||$. Эта метрика показывает величину трансформации одной точки в другую
\item Строится матрица трансформаций $T$:

$t_{ij}=d_{ij}+min(T_{i-1,j-1},T_{i-1,j},T_{i,j-1})$. Эта матрица позволяет найти \missing{уточнить} локальные оптимальные пути трансформаций для последовательностей соседних точек
\item Наконец, находится такая последовательность индексов $w_k=(i,j)_k$, что:
  \begin{itemize}
  \item начальный индекс - (1,1), конечный - (N,M)
  \item $i_{k+1}-i_k \leq 1, j_{k+1}-j_k \leq 1$ - условие непрерывности пути
  \item последовательности индексов $i, j$ не убывают (путь не возвращается в пройденные точки)
  \item суммарный вес $d_{ij}$ минимален среди всех допустимых вариантов
  \end{itemize}
\end{enumerate}

Результат работы алгоритма - оптимальный путь трансформации одного временного ряда в другой. Длина пути (суммарный вес элементов $d_k$, входящих в него), используется как метрика расстояния между рядами. 

\missing{для трёхмерных рядов - поправить код}

\missing{изображения}

\missing{дополнить}
Время работы алгоритма - $O(M*N)$
есть оптимизации

\subsection{Метод k ближайших соседей}

После вычисления метрик можно производить классификацию объектов, основываясь на расстоянии между ними. Одним из наиболее простых методов является алгоритм k ближайших соседей: объекту присваивается тот же класс, что имеет большинство его соседей (т.е. объектов, расстояние до которых минимально).

\missing{дописать}


\section{Алгоритмы, основанные на признаковом описании}

\defn{Признаком} называется результат измерения некоторой характеристики объекта. Формально  — это отображение $f: X\to D_f $, где D\_f — множество допустимых значений признака. Вектор $\bigl( f_1(x),\ldots,f_n(x) \bigr)$ называется \defn{признаковым описанием} объекта $x \in X$ \cite{features_def}. Матрица, состоящая из строк - признаковых описаний объектов, является стандартным видом представления исходных данных во многих алгоритмах машинного обучения. 

Несмотря на то, что временные ряды можно непосредственно рассматривать как наборы признаков, часто имеет смысл выделять другие признаки (выполнять т.н. \defn{feature extraction}) по разным причинам: разные длины рядов, наличие шума и т.д.


\subsection{Классификация на основе коэффициентов авторегрессии}

Модели авторегрессии широко используются в статистике для анализа и прогнозирования стационарных рядов. В их основе лежит представление временного ряда как выхода линейного фильтра, на вход которого подана последовательность нормально распределённых случайных величин:

$z_t=\mu+a_t+\phi_1a_{t-1}+\phi_2a_{t-2}...=\mu+\Phi(B)a_t$, 

где $z_t$ - элемент ряда с индексом $t$, $\mu$ - среднее значение (уровень) ряда, $a_i$ - случайная величина, действующая в момент времени $i$, $\Psi(B)=1+\psi B+\psi_2 B^2+...$ - передаточная функция фильтра, $B$ - лаговый оператор: $Bz_t=z_{t-1}$.

На практике используются модели, представляющие ряд в виде регрессии от конечного числа величин\cite{bj_ts}: 

\begin{itemize}
\item модель авторегресии (\defn{AR-модель}): 

$z_t=\mu+\sum_{i=1}^p \phi_iB^i z_t + a_t$. В данном случае очередное значение ряда линейно зависит от предыдущих $p$ значений ряда
\item модель скользящего среднего (\defn{MA-модель}):

$z_t=\mu + a_t + \sum_{i=1}^q -\theta_iB^i a_t $. В данном случае очередное значение ряда линейно зависит от $q$ предыдущих случайных величин
\item модель авторегрессии-случайного среднего (\defn{ARMA-модель}) представляет собой комбинацию указанных выше методов:

$z_t=\mu + a_t + \sum_{i=1}^p \phi_iB^i z_t + \sum_{i=1}^q -\theta_iB^i a_t $. Применение этой модели позволяет снизить суммарное число параметров, что упрощает дальнейший анализ
\end{itemize}

В качестве признаков объекта могут использоваться параметры $\phi_i, \theta_i, \mu$, дисперсия случайной величины $a_i$ - $\sigma^2$, а также порядки моделей.

\missing{перенести этот блок в общую часть?}
Обратим внимание на то, что в решаемой задаче временные ряды представляют собой последовательности векторов. Соответственно, необходимо каким-либо образом учитывать все элементы этих векторов:

\begin{itemize}
\item Наиболее простой вариант - предположить, что временные ряды, состоящие из отдельных элементов векторов (x, y, z), представляют собой независимые случайные процессы. В этом случае эти ряды можно анализировать по отдельности, а общая модель будет описываться конкатенацией параметров полученных подмоделей\missing{перефразировать}.
\item Учесть зависимости между рядами отдельных элементов. Для методов авторегрессии это, в частности, означает, что модель теперь представляется следующим образом (на примере AR-модели):

$U_t=M+\sum_{i=1}^p \Phi_iB^i U_t + A_t$,  где:

 $U_t=\colthree{x_t}{y_t}{z_t}$, $Phi_i=\mxthree{\phi_{xx}}{\phi_{yx}}{\phi_{zx}}{\phi_{xy}}{\phi_{yy}}{\phi_{zy}}{\phi_{xz}}{\phi_{yz}}{\phi_{zz}}$, 
 $A_t=\colthree{a_x}{a_y}{a_z}$

Модели, реализующие это, называются VAR(VMA,VARMA)-моделями. 
\end{itemize}





\missing{пруфы}
\missing{реализовать честный varma}


% ?  ?






