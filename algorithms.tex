
\chapter{Обзор алгоритмов решения задачи}

В наиболее абстрактном виде задача описывается следующим образом. 

Временные ряды представлены конечными последовательностями значений: $S=s_1,s_2...s_n$, где $s_i$ - вектор значений $\left\{x_i,y_i,z_i\right\}$. Дан конечный набор классов $C=c_1,c_2...c_m$. Известно, что каждый ряд принадлежит одному и только одному классу. 

Для некоторого множества $L$ рядов известна принадлежность ряда к классу - такое множество назовём \defn{обучающей выборкой}. Также имеется множество $T$ рядов, принадлежность которых к классам неизвестна - это \defn{тестовая выборка}.

Необходимо по данной обучающей выборке классифицировать элементы тестовой выборки с максимальной точностью.

\missing{confmat ?}. 

\missing{другие метрики}


\section{Алгоритмы, основанные на метриках расстояния}

Существует класс алгоритмов, основанных на вычислении расстояния (\defn{метрики}) между классифицируемыми объектами. 

\subsection{Алгоритм динамической трансформации временной шкалы (DTW)}

Основная идея алгоритма DTW - минимизация эффекта сдвигов и искажений сигналов во времени путём трансформации одного временного ряда в другой. Это позволяет находить сигналы схожих форм с разными фазами\cite{dtw_review}. 

Пусть даны два временных ряда $X=x_1,x_2...x_N$ и $Y=y_1,y_2...y_M$. Элементы рядов $x_i,y_j$ принадлежат одному пространству \missing{уточнить} $\Phi$, для которого введена метрика $d: \Phi \times \Phi \to R$. 

Алгоритм DTW заключается в следующем:

\begin{enumerate}
\item Строится матрица $D$, состоящая из значений метрик для всех возможных пар элементов рядов: $d_{ij}=|| x_i-y_j ||$. Эта метрика показывает величину трансформации одной точки в другую
\item Строится матрица трансформаций $T$:

$t_{ij}=d_{ij}+min(T_{i-1,j-1},T_{i-1,j},T_{i,j-1})$. Эта матрица позволяет найти \missing{уточнить} локальные оптимальные пути трансформаций для последовательностей соседних точек
\item Наконец, находится такая последовательность индексов $w_k=(i,j)_k$, что:
  \begin{itemize}
  \item начальный индекс - (1,1), конечный - (N,M)
  \item $i_{k+1}-i_k \leq 1, j_{k+1}-j_k \leq 1$ - условие непрерывности пути
  \item последовательности индексов $i, j$ не убывают (путь не возвращается в пройденные точки)
  \item суммарный вес $d_{ij}$ минимален среди всех допустимых вариантов
  \end{itemize}
\end{enumerate}

Результат работы алгоритма - оптимальный путь трансформации одного временного ряда в другой. Длина пути (суммарный вес элементов $d_k$, входящих в него), используется как метрика расстояния между рядами. 

\missing{для трёхмерных рядов - поправить код}

\missing{изображения}

\missing{дополнить}
Время работы алгоритма - $O(M*N)$
есть оптимизации

\subsection{Метод k ближайших соседей}

После вычисления метрик можно производить классификацию объектов, основываясь на расстоянии между ними. Одним из наиболее простых методов является алгоритм k ближайших соседей: объекту присваивается тот же класс, что имеет большинство его соседей (т.е. объектов, расстояние до которых минимально).

\missing{дописать}


\section{Алгоритмы, основанные на признаковом описании}

\defn{Признаком} называется результат измерения некоторой характеристики объекта. Формально  — это отображение $f: X\to D_f $, где D\_f — множество допустимых значений признака. Вектор $\bigl( f_1(x),\ldots,f_n(x) \bigr)$ называется \defn{признаковым описанием} объекта $x \in X$ \cite{features_def}. Матрица, состоящая из строк - признаковых описаний объектов, является стандартным видом представления исходных данных во многих алгоритмах машинного обучения. 

Несмотря на то, что временные ряды можно непосредственно рассматривать как наборы признаков, часто имеет смысл выделять другие признаки (выполнять т.н. \defn{feature extraction}) по разным причинам: разные длины рядов, наличие шума и т.д.


\subsection{Классификация на основе коэффициентов авторегрессии}

Модели авторегрессии широко используются в статистике для анализа и прогнозирования стационарных рядов. В их основе лежит представление временного ряда как выхода линейного фильтра, на вход которого подана последовательность нормально распределённых случайных величин:

$z_t=\mu+a_t+\phi_1a_{t-1}+\phi_2a_{t-2}...=\mu+\Phi(B)a_t$, 

где $z_t$ - элемент ряда с индексом $t$, $\mu$ - среднее значение (уровень) ряда, $a_i$ - случайная величина, действующая в момент времени $i$, $\Psi(B)=1+\psi B+\psi_2 B^2+...$ - передаточная функция фильтра, $B$ - лаговый оператор: $Bz_t=z_{t-1}$.

На практике используются модели, представляющие ряд в виде регрессии от конечного числа величин\cite{bj_ts}: 

\begin{itemize}
\item модель авторегресии (\defn{AR-модель}): 

$z_t=\mu+\sum_{i=1}^p \phi_iB^i z_t + a_t$. В данном случае очередное значение ряда линейно зависит от предыдущих $p$ значений ряда
\item модель скользящего среднего (\defn{MA-модель}):

$z_t=\mu + a_t + \sum_{i=1}^q -\theta_iB^i a_t $. В данном случае очередное значение ряда линейно зависит от $q$ предыдущих случайных величин
\item модель авторегрессии-случайного среднего (\defn{ARMA-модель}) представляет собой комбинацию указанных выше методов:

$z_t=\mu + a_t + \sum_{i=1}^p \phi_iB^i z_t + \sum_{i=1}^q -\theta_iB^i a_t $. Применение этой модели позволяет снизить суммарное число параметров, что упрощает дальнейший анализ
\end{itemize}

В качестве признаков объекта могут использоваться параметры $\phi_i, \theta_i, \mu$, дисперсия случайной величины $a_i$ - $\sigma^2$, а также порядки моделей.

\missing{перенести этот блок в общую часть?}
Обратим внимание на то, что в решаемой задаче временные ряды представляют собой последовательности векторов. Соответственно, необходимо каким-либо образом учитывать все элементы этих векторов:

\begin{itemize}
\item Наиболее простой вариант - предположить, что временные ряды, состоящие из отдельных элементов векторов (x, y, z), представляют собой независимые случайные процессы. В этом случае эти ряды можно анализировать по отдельности, а общая модель будет описываться конкатенацией параметров полученных подмоделей\missing{перефразировать}.
\item Учесть зависимости между рядами отдельных элементов. Для методов авторегрессии это, в частности, означает, что модель теперь представляется следующим образом (на примере AR-модели):

$U_t=M+\sum_{i=1}^p \Phi_iB^i U_t + A_t$,  где:

 $U_t=\colthree{x_t}{y_t}{z_t}$, $Phi_i=\mxthree{\phi_{xx}}{\phi_{yx}}{\phi_{zx}}{\phi_{xy}}{\phi_{yy}}{\phi_{zy}}{\phi_{xz}}{\phi_{yz}}{\phi_{zz}}$, 
 $A_t=\colthree{a_x}{a_y}{a_z}$

Модели, реализующие это, называются VAR(VMA,VARMA)-моделями. 
\end{itemize}


\missing{пруфы}
\missing{реализовать честный varma}


\subsection{Классификация на основе параметров скрытой марковской модели}

Дискретная марковская модель описывает систему, которая может в каждый данный дискретный момент времени находиться в одном из состояний $V=\{v_1...v_M\}$. Переход из одного состояния в другое зависит только от текущего состояния; таким образом, модель описывается матрицей $A: a_{ij}=P(v_i \to v_j)$. Это - наблюдаемая марковская модель: состояние системы является наблюдаемой величиной. 

Более общим и практичным подходом является аппарат \defn{скрытых марковских моделей} (СММ) - обобщение описанной концепции. В нём наблюдаемые события $v_k$ являются некоторой вероятностной функцией текущего состояния $s_j$, что описывается матрицей $B: b_j(k)=P[v_k | s_j]$. Кроме того, задаётся распределение вероятностей начального состояния $\Pi: \pi_i=P[q_1=s_i]$. 

Можно использовать СММ для моделирования временных рядов. Применительно к решаемой задаче наблюдаемые события $v_k$ - векторы-элементы временного ряда. Если удастся задать параметры модели так, чтобы последовательность наблюдаемых событий соответствовала данному временному ряду, то эти параметры можно будет использовать в качестве признаков для дальнейшей классификации.

Для нахождения оптимальных (максимизирующих вероятность наблюдения требуемой последовательности) параметров СММ используется алгоритм Баума-Велша.\missing{добавить ссылки} \missing{добавить обозначения} В нём используются выражения $\alpha_t(i)=P(O_1, ... O_t , Q_t=i)$ и $\beta_t(i)=P(O_{t+1}, ... O_T , Q_t=i)$ - т.н. прямые и обратные переменные. Для их вычисления используются процедуры прямого и обратного хода соответственно \cite{ДОБАВИТЬ}. Они показывают для выбранных момента времени и скрытого состояния, какова вероятность появления наблюдаемой последовательности до и после момента $t$ соответственно. 

На их основе вводятся также:
\begin{itemize}
\item $\gamma_t(i)=P(q_t=i | O)=\dfrac{\alpha_t(i)\beta_t(i)}{\sum_k^N\alpha_t(k)\beta_t(k)}$ - вероятность нахождения в $i$ состоянии в момент времени $t$. Если эту величину просуммировать по всем $t$, то результат можно рассмотреть как ожидаемое количество переходов из состояния $i$
\item $\varepsilon_t(i,j)=P(q_t=i, q_{t+1}=j | O)=\dfrac{\alpha_t(i)a_{ij}b_j(O_{t+1})\beta_{t+1}(i)}{\sum_u^N\sum_v^N\alpha_t(u)a_{uv}b_v(O_{t+1})\beta_{t+1}(v)}$ - вероятность перехода из $i$ в $j$ в момент времени $t$. Если эту величину просуммировать по всем $t$, то результат можно рассмотреть как ожидаемое количество переходов из состояния $i$ в $j$
\end{itemize}

На основе этих формул можно произвести переоценку параметров $\pi, A, B$ и рассмотреть их смысл с точки зрения частоты переходов между состояниями:

\begin{itemize}
\item $\overline{\pi_i}=\gamma_1(i)$ - ожидаемое число переходов из $i$ в момент $t=1$
\item $\overline{a_{ij}}=\dfrac{\sum_t^T\varepsilon_t(i,j)}{\sum_t^T\gamma_t(i)}$ - ожидаемое число переходов из $i$ в $j$
\item $\overline{b_{j}(k)}=\dfrac{\sum_t^T\gamma_t(i)*b_j(k)}{\sum_t^T\gamma_t(i)}$\missing{уточнить} - ожидаемое число переходов из $i$ при наблюдении $k$
\end{itemize}

Баумом и его коллегами было показано, что этот алгоритм сходится к локальному оптимуму значений параметров.\missing{добавить ссылку из статьи}


% ?  ?






